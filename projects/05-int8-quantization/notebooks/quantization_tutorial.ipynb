{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INT8 Quantization Tutorial\n",
    "\n",
    "## A Comprehensive Guide to Neural Network Quantization\n",
    "\n",
    "This notebook provides a hands-on tutorial for implementing INT8 quantization using both Post-Training Quantization (PTQ) and Quantization-Aware Training (QAT).\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "By the end of this tutorial, you will understand:\n",
    "1. Fundamental concepts of neural network quantization\n",
    "2. How to implement PTQ using TensorRT\n",
    "3. How to perform QAT with PyTorch\n",
    "4. Layer-wise sensitivity analysis techniques\n",
    "5. Mixed precision optimization strategies\n",
    "6. Performance evaluation and comparison methods\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "- Basic understanding of neural networks\n",
    "- Familiarity with PyTorch\n",
    "- NVIDIA GPU with TensorRT support (optional but recommended)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "First, let's import the necessary libraries and setup our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Project imports\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "from calibration_dataset import create_calibration_dataloader\n",
    "from ptq_tensorrt import quantize_pytorch_model_ptq\n",
    "from qat_pytorch import train_qat_model\n",
    "from sensitivity_analysis import analyze_model_sensitivity\n",
    "from mixed_precision import optimize_mixed_precision, PrecisionConstraints\n",
    "from accuracy_evaluation import evaluate_quantized_models\n",
    "from compare_methods import compare_quantization_methods\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Setup device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Setup plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Understanding Quantization Fundamentals\n",
    "\n",
    "### 2.1 What is Quantization?\n",
    "\n",
    "Quantization is the process of reducing the precision of weights and activations in neural networks from 32-bit floating point (FP32) to lower bit representations like 8-bit integers (INT8)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate quantization fundamentals\n",
    "def demonstrate_quantization():\n",
    "    \"\"\"Visualize the quantization process.\"\"\"\n",
    "    \n",
    "    # Generate sample FP32 weights\n",
    "    fp32_weights = torch.randn(1000) * 2.0  # Range roughly [-6, 6]\n",
    "    \n",
    "    # Simulate INT8 quantization\n",
    "    def quantize_tensor(tensor, bits=8):\n",
    "        \"\"\"Simple symmetric quantization.\"\"\"\n",
    "        # Calculate scale\n",
    "        max_val = tensor.abs().max()\n",
    "        scale = max_val / (2**(bits-1) - 1)\n",
    "        \n",
    "        # Quantize\n",
    "        quantized = torch.round(tensor / scale).clamp(-(2**(bits-1)), 2**(bits-1)-1)\n",
    "        \n",
    "        # Dequantize\n",
    "        dequantized = quantized * scale\n",
    "        \n",
    "        return dequantized, quantized, scale\n",
    "    \n",
    "    # Apply quantization\n",
    "    dequantized_weights, quantized_weights, scale = quantize_tensor(fp32_weights)\n",
    "    \n",
    "    # Calculate error\n",
    "    quantization_error = torch.abs(fp32_weights - dequantized_weights)\n",
    "    mse = torch.mean(quantization_error ** 2)\n",
    "    \n",
    "    # Visualize results\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Original distribution\n",
    "    axes[0,0].hist(fp32_weights.numpy(), bins=50, alpha=0.7, label='FP32', color='blue')\n",
    "    axes[0,0].set_title('Original FP32 Weights')\n",
    "    axes[0,0].set_xlabel('Value')\n",
    "    axes[0,0].set_ylabel('Count')\n",
    "    \n",
    "    # Quantized distribution\n",
    "    axes[0,1].hist(quantized_weights.numpy(), bins=50, alpha=0.7, label='INT8', color='orange')\n",
    "    axes[0,1].set_title(f'Quantized INT8 Weights (scale={scale:.4f})')\n",
    "    axes[0,1].set_xlabel('Quantized Value')\n",
    "    axes[0,1].set_ylabel('Count')\n",
    "    \n",
    "    # Comparison\n",
    "    axes[1,0].scatter(fp32_weights[:100], dequantized_weights[:100], alpha=0.6)\n",
    "    axes[1,0].plot([-6, 6], [-6, 6], 'r--', label='Perfect Match')\n",
    "    axes[1,0].set_xlabel('Original FP32')\n",
    "    axes[1,0].set_ylabel('Dequantized INT8')\n",
    "    axes[1,0].set_title('FP32 vs Dequantized Comparison')\n",
    "    axes[1,0].legend()\n",
    "    \n",
    "    # Error distribution\n",
    "    axes[1,1].hist(quantization_error.numpy(), bins=30, alpha=0.7, color='red')\n",
    "    axes[1,1].set_title(f'Quantization Error (MSE={mse:.6f})')\n",
    "    axes[1,1].set_xlabel('Absolute Error')\n",
    "    axes[1,1].set_ylabel('Count')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Quantization Statistics:\")\n",
    "    print(f\"  Original range: [{fp32_weights.min():.3f}, {fp32_weights.max():.3f}]\")\n",
    "    print(f\"  Quantization scale: {scale:.6f}\")\n",
    "    print(f\"  Mean absolute error: {quantization_error.mean():.6f}\")\n",
    "    print(f\"  Mean squared error: {mse:.6f}\")\n",
    "    print(f\"  Storage reduction: {32/8:.0f}x (FP32 ‚Üí INT8)\")\n",
    "\n",
    "# Run demonstration\n",
    "demonstrate_quantization()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Types of Quantization\n",
    "\n",
    "There are two main approaches to quantization:\n",
    "\n",
    "1. **Post-Training Quantization (PTQ)**: Quantize a pre-trained model without retraining\n",
    "2. **Quantization-Aware Training (QAT)**: Simulate quantization during training for better accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table\n",
    "import pandas as pd\n",
    "\n",
    "comparison_data = {\n",
    "    'Aspect': ['Training Required', 'Time to Deploy', 'Accuracy', 'Computational Cost', 'Use Case'],\n",
    "    'Post-Training Quantization (PTQ)': [\n",
    "        'No', 'Fast (minutes)', 'Good (1-3% drop)', 'Low', 'Quick deployment'\n",
    "    ],\n",
    "    'Quantization-Aware Training (QAT)': [\n",
    "        'Yes', 'Slow (hours/days)', 'Better (0.5-1% drop)', 'High', 'Maximum accuracy'\n",
    "    ]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"PTQ vs QAT Comparison:\")\n",
    "print(comparison_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Loading and Preparing Models\n",
    "\n",
    "Let's start with a practical example using a pre-trained ResNet model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained models for demonstration\n",
    "def load_demo_models():\n",
    "    \"\"\"Load models for quantization demonstration.\"\"\"\n",
    "    \n",
    "    models_dict = {\n",
    "        'resnet18': models.resnet18(pretrained=True),\n",
    "        'mobilenet_v2': models.mobilenet_v2(pretrained=True)\n",
    "    }\n",
    "    \n",
    "    # Set to evaluation mode\n",
    "    for model in models_dict.values():\n",
    "        model.eval()\n",
    "    \n",
    "    return models_dict\n",
    "\n",
    "# Load models\n",
    "demo_models = load_demo_models()\n",
    "\n",
    "# Display model information\n",
    "for name, model in demo_models.items():\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f\"\\n{name.upper()} Model:\")\n",
    "    print(f\"  Total parameters: {total_params:,}\")\n",
    "    print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "    print(f\"  Model size (approx): {total_params * 4 / (1024**2):.1f} MB\")\n",
    "\n",
    "# Select primary model for tutorial\n",
    "primary_model = demo_models['resnet18']\n",
    "print(f\"\\nUsing ResNet18 as primary model for this tutorial.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Creating Calibration Dataset\n",
    "\n",
    "For PTQ, we need a representative calibration dataset to determine optimal quantization parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic calibration dataset for demonstration\n",
    "# In practice, you would use real ImageNet data\n",
    "\n",
    "class SyntheticImageNet:\n",
    "    \"\"\"Synthetic ImageNet-like dataset for demonstration.\"\"\"\n",
    "    \n",
    "    def __init__(self, size=1000, image_size=224):\n",
    "        self.size = size\n",
    "        self.image_size = image_size\n",
    "        \n",
    "        # Standard ImageNet transforms\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225]\n",
    "            )\n",
    "        ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Generate synthetic image with realistic statistics\n",
    "        image = torch.randn(3, self.image_size, self.image_size) * 0.2 + 0.5\n",
    "        image = self.transform(image)\n",
    "        \n",
    "        # Random label\n",
    "        label = torch.randint(0, 1000, (1,)).item()\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "# Create synthetic datasets\n",
    "calibration_dataset = SyntheticImageNet(size=1000)\n",
    "validation_dataset = SyntheticImageNet(size=5000)\n",
    "training_dataset = SyntheticImageNet(size=2000)  # Small for demo\n",
    "\n",
    "# Create data loaders\n",
    "calibration_loader = DataLoader(calibration_dataset, batch_size=32, shuffle=False)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=64, shuffle=False)\n",
    "training_loader = DataLoader(training_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "print(f\"Created synthetic datasets:\")\n",
    "print(f\"  Calibration: {len(calibration_dataset)} samples\")\n",
    "print(f\"  Validation: {len(validation_dataset)} samples\")\n",
    "print(f\"  Training: {len(training_dataset)} samples\")\n",
    "\n",
    "# Visualize sample data\n",
    "sample_batch, sample_labels = next(iter(calibration_loader))\n",
    "print(f\"\\nSample batch shape: {sample_batch.shape}\")\n",
    "print(f\"Data range: [{sample_batch.min():.3f}, {sample_batch.max():.3f}]\")\n",
    "print(f\"Label range: [{min(sample_labels)}, {max(sample_labels)}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Baseline Model Evaluation\n",
    "\n",
    "Before quantization, let's establish baseline performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate baseline performance\n",
    "def evaluate_model_performance(model, dataloader, device, model_name=\"Model\"):\n",
    "    \"\"\"Evaluate model accuracy and inference speed.\"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    correct_top1 = 0\n",
    "    correct_top5 = 0\n",
    "    total = 0\n",
    "    inference_times = []\n",
    "    \n",
    "    print(f\"Evaluating {model_name}...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, targets) in enumerate(dataloader):\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "            \n",
    "            # Time inference\n",
    "            start_time = time.time()\n",
    "            outputs = model(data)\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.synchronize()\n",
    "            inference_time = (time.time() - start_time) * 1000  # Convert to ms\n",
    "            inference_times.append(inference_time)\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            _, pred_top1 = torch.max(outputs, 1)\n",
    "            correct_top1 += pred_top1.eq(targets).sum().item()\n",
    "            \n",
    "            # Top-5 accuracy\n",
    "            _, pred_top5 = torch.topk(outputs, 5, dim=1)\n",
    "            correct_top5 += pred_top5.eq(targets.view(-1, 1).expand_as(pred_top5)).sum().item()\n",
    "            \n",
    "            total += targets.size(0)\n",
    "            \n",
    "            if batch_idx >= 50:  # Limit for demo\n",
    "                break\n",
    "    \n",
    "    # Calculate metrics\n",
    "    top1_acc = 100.0 * correct_top1 / total\n",
    "    top5_acc = 100.0 * correct_top5 / total\n",
    "    avg_inference_time = np.mean(inference_times)\n",
    "    \n",
    "    # Model size estimation\n",
    "    model_size = sum(p.numel() * 4 for p in model.parameters()) / (1024**2)  # MB\n",
    "    \n",
    "    results = {\n",
    "        'top1_accuracy': top1_acc,\n",
    "        'top5_accuracy': top5_acc,\n",
    "        'avg_inference_time_ms': avg_inference_time,\n",
    "        'model_size_mb': model_size,\n",
    "        'total_samples': total\n",
    "    }\n",
    "    \n",
    "    print(f\"Results for {model_name}:\")\n",
    "    print(f\"  Top-1 Accuracy: {top1_acc:.2f}%\")\n",
    "    print(f\"  Top-5 Accuracy: {top5_acc:.2f}%\")\n",
    "    print(f\"  Avg Inference Time: {avg_inference_time:.2f} ms\")\n",
    "    print(f\"  Model Size: {model_size:.1f} MB\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Evaluate baseline model\n",
    "baseline_results = evaluate_model_performance(\n",
    "    primary_model, validation_loader, device, \"ResNet18 FP32 Baseline\"\n",
    ")\n",
    "\n",
    "# Store for comparison\n",
    "all_results = {'FP32_Baseline': baseline_results}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Layer-wise Sensitivity Analysis\n",
    "\n",
    "Understanding which layers are sensitive to quantization helps optimize mixed precision strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform sensitivity analysis (simplified for tutorial)\n",
    "def simple_sensitivity_analysis(model, dataloader, device, num_samples=500):\n",
    "    \"\"\"Simplified sensitivity analysis for demonstration.\"\"\"\n",
    "    \n",
    "    # Get quantizable layers\n",
    "    quantizable_layers = []\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, (nn.Conv2d, nn.Linear)):\n",
    "            quantizable_layers.append(name)\n",
    "    \n",
    "    print(f\"Found {len(quantizable_layers)} quantizable layers\")\n",
    "    \n",
    "    # For demo, simulate sensitivity scores\n",
    "    # In practice, you would run actual layer-wise quantization\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    sensitivity_scores = {}\n",
    "    for layer_name in quantizable_layers:\n",
    "        # Simulate: first/last layers more sensitive, middle layers less sensitive\n",
    "        if 'conv1' in layer_name or 'fc' in layer_name:\n",
    "            sensitivity = np.random.uniform(1.5, 3.0)  # High sensitivity\n",
    "        elif 'layer1' in layer_name:\n",
    "            sensitivity = np.random.uniform(0.8, 1.2)  # Medium sensitivity  \n",
    "        else:\n",
    "            sensitivity = np.random.uniform(0.1, 0.8)  # Low sensitivity\n",
    "        \n",
    "        sensitivity_scores[layer_name] = sensitivity\n",
    "    \n",
    "    return sensitivity_scores\n",
    "\n",
    "# Run sensitivity analysis\n",
    "print(\"Running layer-wise sensitivity analysis...\")\n",
    "sensitivity_scores = simple_sensitivity_analysis(\n",
    "    primary_model, validation_loader, device\n",
    ")\n",
    "\n",
    "# Visualize sensitivity scores\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Bar plot of sensitivity scores\n",
    "layers = list(sensitivity_scores.keys())\n",
    "scores = list(sensitivity_scores.values())\n",
    "\n",
    "# Shorten layer names for display\n",
    "short_names = [name.split('.')[-1] if '.' in name else name for name in layers]\n",
    "\n",
    "bars = ax1.bar(range(len(short_names)), scores, \n",
    "               color=['red' if s > 1.0 else 'orange' if s > 0.5 else 'green' for s in scores])\n",
    "ax1.set_xlabel('Layer Index')\n",
    "ax1.set_ylabel('Sensitivity Score (Accuracy Drop %)')\n",
    "ax1.set_title('Layer-wise Quantization Sensitivity')\n",
    "ax1.set_xticks(range(len(short_names)))\n",
    "ax1.set_xticklabels(short_names, rotation=45)\n",
    "\n",
    "# Histogram of sensitivity distribution\n",
    "ax2.hist(scores, bins=10, alpha=0.7, edgecolor='black')\n",
    "ax2.axvline(np.mean(scores), color='red', linestyle='--', \n",
    "            label=f'Mean: {np.mean(scores):.2f}')\n",
    "ax2.set_xlabel('Sensitivity Score')\n",
    "ax2.set_ylabel('Number of Layers')\n",
    "ax2.set_title('Sensitivity Distribution')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print top sensitive layers\n",
    "sorted_sensitivity = sorted(sensitivity_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "print(\"\\nTop 5 most sensitive layers:\")\n",
    "for i, (layer_name, score) in enumerate(sorted_sensitivity[:5]):\n",
    "    print(f\"  {i+1}. {layer_name}: {score:.3f}% accuracy drop\")\n",
    "\n",
    "# Classification by sensitivity\n",
    "high_sens = sum(1 for s in scores if s > 1.0)\n",
    "medium_sens = sum(1 for s in scores if 0.5 < s <= 1.0)\n",
    "low_sens = sum(1 for s in scores if s <= 0.5)\n",
    "\n",
    "print(f\"\\nSensitivity classification:\")\n",
    "print(f\"  High sensitivity (>1.0%): {high_sens} layers\")\n",
    "print(f\"  Medium sensitivity (0.5-1.0%): {medium_sens} layers\")\n",
    "print(f\"  Low sensitivity (‚â§0.5%): {low_sens} layers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Post-Training Quantization (PTQ)\n",
    "\n",
    "Now let's implement PTQ using TensorRT-style calibration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate PTQ process (actual TensorRT integration would require GPU setup)\n",
    "def simulate_ptq_quantization(model, calibration_loader, method='entropy'):\n",
    "    \"\"\"Simulate PTQ quantization for demonstration.\"\"\"\n",
    "    \n",
    "    print(f\"Simulating PTQ quantization with {method} calibration...\")\n",
    "    \n",
    "    # In practice, this would use TensorRT calibration\n",
    "    # For demo, we'll simulate the quantization process\n",
    "    \n",
    "    import copy\n",
    "    quantized_model = copy.deepcopy(model)\n",
    "    \n",
    "    # Simulate quantization by adding noise to weights\n",
    "    # This mimics the effect of INT8 quantization\n",
    "    quantization_noise_level = 0.02 if method == 'entropy' else 0.04\n",
    "    \n",
    "    for name, param in quantized_model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            # Add quantization noise\n",
    "            noise = torch.randn_like(param) * param.std() * quantization_noise_level\n",
    "            param.data += noise\n",
    "    \n",
    "    # Simulate calibration time\n",
    "    calibration_time = 120.0 if method == 'entropy' else 60.0  # seconds\n",
    "    \n",
    "    # Simulate compression and speedup\n",
    "    compression_ratio = 3.8 if method == 'entropy' else 3.5\n",
    "    speedup_factor = 2.3 if method == 'entropy' else 2.1\n",
    "    \n",
    "    return quantized_model, {\n",
    "        'calibration_time': calibration_time,\n",
    "        'compression_ratio': compression_ratio,\n",
    "        'speedup_factor': speedup_factor,\n",
    "        'method': method\n",
    "    }\n",
    "\n",
    "# Test both calibration methods\n",
    "ptq_methods = ['entropy', 'minmax']\n",
    "ptq_models = {}\n",
    "ptq_info = {}\n",
    "\n",
    "for method in ptq_methods:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Testing PTQ with {method} calibration\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Simulate PTQ\n",
    "    ptq_model, info = simulate_ptq_quantization(\n",
    "        primary_model, calibration_loader, method\n",
    "    )\n",
    "    \n",
    "    # Evaluate quantized model\n",
    "    ptq_results = evaluate_model_performance(\n",
    "        ptq_model, validation_loader, device, \n",
    "        f\"PTQ ({method})\"\n",
    "    )\n",
    "    \n",
    "    # Calculate metrics relative to baseline\n",
    "    accuracy_drop = baseline_results['top1_accuracy'] - ptq_results['top1_accuracy']\n",
    "    \n",
    "    print(f\"\\nPTQ {method} Summary:\")\n",
    "    print(f\"  Accuracy drop: {accuracy_drop:.2f}%\")\n",
    "    print(f\"  Compression ratio: {info['compression_ratio']:.1f}x\")\n",
    "    print(f\"  Speedup factor: {info['speedup_factor']:.1f}x\")\n",
    "    print(f\"  Calibration time: {info['calibration_time']:.0f}s\")\n",
    "    \n",
    "    # Store results\n",
    "    model_key = f'PTQ_{method}'\n",
    "    ptq_models[model_key] = ptq_model\n",
    "    ptq_info[model_key] = info\n",
    "    all_results[model_key] = ptq_results\n",
    "    all_results[model_key]['accuracy_drop'] = accuracy_drop\n",
    "    all_results[model_key]['compression_ratio'] = info['compression_ratio']\n",
    "    all_results[model_key]['speedup_factor'] = info['speedup_factor']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Quantization-Aware Training (QAT)\n",
    "\n",
    "QAT typically provides better accuracy by simulating quantization during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate QAT process\n",
    "def simulate_qat_training(model, train_loader, val_loader, num_epochs=3):\n",
    "    \"\"\"Simulate QAT training process for demonstration.\"\"\"\n",
    "    \n",
    "    print(f\"Simulating QAT training for {num_epochs} epochs...\")\n",
    "    \n",
    "    import copy\n",
    "    qat_model = copy.deepcopy(model)\n",
    "    qat_model.train()\n",
    "    \n",
    "    # Setup optimizer (using smaller learning rate for fine-tuning)\n",
    "    optimizer = torch.optim.Adam(qat_model.parameters(), lr=0.0001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    training_history = {'train_loss': [], 'val_acc': []}\n",
    "    \n",
    "    # Simulate training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        # Training phase\n",
    "        qat_model.train()\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = qat_model(data)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            # Add simulated quantization noise during training\n",
    "            quantization_loss = 0.001 * sum(\n",
    "                torch.norm(p) for p in qat_model.parameters()\n",
    "            )\n",
    "            total_loss_with_quant = loss + quantization_loss\n",
    "            \n",
    "            total_loss_with_quant.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            \n",
    "            # Limit batches for demo\n",
    "            if batch_idx >= 20:\n",
    "                break\n",
    "        \n",
    "        avg_loss = total_loss / num_batches\n",
    "        training_history['train_loss'].append(avg_loss)\n",
    "        \n",
    "        # Validation phase\n",
    "        qat_model.eval()\n",
    "        val_results = evaluate_model_performance(\n",
    "            qat_model, val_loader, device, f\"QAT Epoch {epoch+1}\"\n",
    "        )\n",
    "        training_history['val_acc'].append(val_results['top1_accuracy'])\n",
    "        \n",
    "        print(f\"  Train Loss: {avg_loss:.4f}\")\n",
    "        print(f\"  Val Accuracy: {val_results['top1_accuracy']:.2f}%\")\n",
    "    \n",
    "    # Convert to quantized model (simulated)\n",
    "    quantized_qat_model = copy.deepcopy(qat_model)\n",
    "    \n",
    "    # Simulate final quantization step\n",
    "    for name, param in quantized_qat_model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            # Less noise than PTQ due to quantization-aware training\n",
    "            noise = torch.randn_like(param) * param.std() * 0.01\n",
    "            param.data += noise\n",
    "    \n",
    "    training_time = 300 * num_epochs  # Simulated training time\n",
    "    \n",
    "    return quantized_qat_model, {\n",
    "        'training_time': training_time,\n",
    "        'training_history': training_history,\n",
    "        'compression_ratio': 3.9,  # Slightly better than PTQ\n",
    "        'speedup_factor': 2.4\n",
    "    }\n",
    "\n",
    "# Run QAT simulation\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Testing Quantization-Aware Training (QAT)\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "qat_model, qat_info = simulate_qat_training(\n",
    "    primary_model, training_loader, validation_loader, num_epochs=2\n",
    ")\n",
    "\n",
    "# Final evaluation of QAT model\n",
    "qat_results = evaluate_model_performance(\n",
    "    qat_model, validation_loader, device, \"Final QAT Model\"\n",
    ")\n",
    "\n",
    "# Calculate metrics\n",
    "qat_accuracy_drop = baseline_results['top1_accuracy'] - qat_results['top1_accuracy']\n",
    "\n",
    "print(f\"\\nQAT Summary:\")\n",
    "print(f\"  Accuracy drop: {qat_accuracy_drop:.2f}%\")\n",
    "print(f\"  Compression ratio: {qat_info['compression_ratio']:.1f}x\")\n",
    "print(f\"  Speedup factor: {qat_info['speedup_factor']:.1f}x\")\n",
    "print(f\"  Training time: {qat_info['training_time']:.0f}s\")\n",
    "\n",
    "# Store results\n",
    "all_results['QAT'] = qat_results\n",
    "all_results['QAT']['accuracy_drop'] = qat_accuracy_drop\n",
    "all_results['QAT']['compression_ratio'] = qat_info['compression_ratio']\n",
    "all_results['QAT']['speedup_factor'] = qat_info['speedup_factor']\n",
    "\n",
    "# Plot training history\n",
    "if len(qat_info['training_history']['train_loss']) > 1:\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    epochs = range(1, len(qat_info['training_history']['train_loss']) + 1)\n",
    "    \n",
    "    ax1.plot(epochs, qat_info['training_history']['train_loss'], 'b-o')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Training Loss')\n",
    "    ax1.set_title('QAT Training Loss')\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    ax2.plot(epochs, qat_info['training_history']['val_acc'], 'r-o')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Validation Accuracy (%)')\n",
    "    ax2.set_title('QAT Validation Accuracy')\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Mixed Precision Optimization\n",
    "\n",
    "Based on sensitivity analysis, we can optimize which layers to keep in higher precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement mixed precision optimization\n",
    "def optimize_mixed_precision_demo(sensitivity_scores):\n",
    "    \"\"\"Demonstrate mixed precision optimization.\"\"\"\n",
    "    \n",
    "    print(\"Optimizing mixed precision assignment...\")\n",
    "    \n",
    "    # Define precision assignment based on sensitivity\n",
    "    precision_assignment = {}\n",
    "    precision_stats = {'FP32': 0, 'FP16': 0, 'INT8': 0}\n",
    "    \n",
    "    for layer_name, sensitivity in sensitivity_scores.items():\n",
    "        if sensitivity > 1.5:  # Very sensitive\n",
    "            precision = 'FP32'\n",
    "        elif sensitivity > 0.8:  # Moderately sensitive\n",
    "            precision = 'FP16'\n",
    "        else:  # Low sensitivity\n",
    "            precision = 'INT8'\n",
    "        \n",
    "        precision_assignment[layer_name] = precision\n",
    "        precision_stats[precision] += 1\n",
    "    \n",
    "    # Calculate estimated metrics\n",
    "    total_layers = len(sensitivity_scores)\n",
    "    int8_ratio = precision_stats['INT8'] / total_layers\n",
    "    fp16_ratio = precision_stats['FP16'] / total_layers\n",
    "    fp32_ratio = precision_stats['FP32'] / total_layers\n",
    "    \n",
    "    # Estimate compression (weighted average)\n",
    "    estimated_compression = (\n",
    "        int8_ratio * 4.0 +  # INT8 gives 4x compression\n",
    "        fp16_ratio * 2.0 +  # FP16 gives 2x compression  \n",
    "        fp32_ratio * 1.0    # FP32 gives no compression\n",
    "    )\n",
    "    \n",
    "    # Estimate accuracy drop (weighted by sensitivity)\n",
    "    estimated_accuracy_drop = sum(\n",
    "        sensitivity * (0.0 if precision_assignment[layer] == 'FP32' \n",
    "                      else 0.1 if precision_assignment[layer] == 'FP16'\n",
    "                      else 1.0)\n",
    "        for layer, sensitivity in sensitivity_scores.items()\n",
    "    ) / total_layers\n",
    "    \n",
    "    return {\n",
    "        'precision_assignment': precision_assignment,\n",
    "        'precision_stats': precision_stats,\n",
    "        'int8_ratio': int8_ratio,\n",
    "        'fp16_ratio': fp16_ratio,\n",
    "        'fp32_ratio': fp32_ratio,\n",
    "        'estimated_compression': estimated_compression,\n",
    "        'estimated_accuracy_drop': estimated_accuracy_drop\n",
    "    }\n",
    "\n",
    "# Run mixed precision optimization\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Mixed Precision Optimization\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "mixed_precision_result = optimize_mixed_precision_demo(sensitivity_scores)\n",
    "\n",
    "# Display results\n",
    "print(f\"\\nPrecision Distribution:\")\n",
    "for precision, count in mixed_precision_result['precision_stats'].items():\n",
    "    ratio = count / len(sensitivity_scores)\n",
    "    print(f\"  {precision}: {count} layers ({ratio:.1%})\")\n",
    "\n",
    "print(f\"\\nEstimated Performance:\")\n",
    "print(f\"  Compression ratio: {mixed_precision_result['estimated_compression']:.1f}x\")\n",
    "print(f\"  Estimated accuracy drop: {mixed_precision_result['estimated_accuracy_drop']:.2f}%\")\n",
    "\n",
    "# Visualize precision assignment\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Precision distribution pie chart\n",
    "labels = list(mixed_precision_result['precision_stats'].keys())\n",
    "sizes = list(mixed_precision_result['precision_stats'].values())\n",
    "colors = ['red', 'orange', 'green']\n",
    "\n",
    "ax1.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
    "ax1.set_title('Precision Distribution')\n",
    "\n",
    "# Layer-wise precision assignment\n",
    "layers = list(sensitivity_scores.keys())\n",
    "precision_values = [mixed_precision_result['precision_assignment'][layer] for layer in layers]\n",
    "precision_colors = {'FP32': 'red', 'FP16': 'orange', 'INT8': 'green'}\n",
    "\n",
    "bar_colors = [precision_colors[p] for p in precision_values]\n",
    "short_names = [name.split('.')[-1] if '.' in name else name for name in layers]\n",
    "\n",
    "ax2.bar(range(len(short_names)), [1]*len(short_names), color=bar_colors)\n",
    "ax2.set_xlabel('Layer Index')\n",
    "ax2.set_ylabel('Assigned Precision')\n",
    "ax2.set_title('Layer-wise Precision Assignment')\n",
    "ax2.set_xticks(range(len(short_names)))\n",
    "ax2.set_xticklabels(short_names, rotation=45)\n",
    "\n",
    "# Add legend\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [Patch(facecolor=color, label=precision) \n",
    "                  for precision, color in precision_colors.items()]\n",
    "ax2.legend(handles=legend_elements)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Store mixed precision results\n",
    "mixed_precision_accuracy = baseline_results['top1_accuracy'] - mixed_precision_result['estimated_accuracy_drop']\n",
    "all_results['Mixed_Precision'] = {\n",
    "    'top1_accuracy': mixed_precision_accuracy,\n",
    "    'accuracy_drop': mixed_precision_result['estimated_accuracy_drop'],\n",
    "    'compression_ratio': mixed_precision_result['estimated_compression'],\n",
    "    'speedup_factor': 2.0,  # Estimated\n",
    "    'model_size_mb': baseline_results['model_size_mb'] / mixed_precision_result['estimated_compression']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Comprehensive Comparison\n",
    "\n",
    "Now let's compare all quantization methods side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive comparison\n",
    "def create_comparison_report(all_results, baseline_results):\n",
    "    \"\"\"Create comprehensive comparison of all methods.\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"COMPREHENSIVE QUANTIZATION COMPARISON\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Prepare comparison data\n",
    "    comparison_data = []\n",
    "    \n",
    "    for method_name, results in all_results.items():\n",
    "        if method_name == 'FP32_Baseline':\n",
    "            row = {\n",
    "                'Method': 'FP32 Baseline',\n",
    "                'Top-1 Acc (%)': f\"{results['top1_accuracy']:.2f}\",\n",
    "                'Accuracy Drop (%)': \"0.00\",\n",
    "                'Model Size (MB)': f\"{results['model_size_mb']:.1f}\",\n",
    "                'Compression': \"1.0x\",\n",
    "                'Inference Time (ms)': f\"{results['avg_inference_time_ms']:.2f}\",\n",
    "                'Speedup': \"1.0x\"\n",
    "            }\n",
    "        else:\n",
    "            row = {\n",
    "                'Method': method_name.replace('_', ' '),\n",
    "                'Top-1 Acc (%)': f\"{results['top1_accuracy']:.2f}\",\n",
    "                'Accuracy Drop (%)': f\"{results.get('accuracy_drop', 0):.2f}\",\n",
    "                'Model Size (MB)': f\"{results.get('model_size_mb', baseline_results['model_size_mb']/3):.1f}\",\n",
    "                'Compression': f\"{results.get('compression_ratio', 3.0):.1f}x\",\n",
    "                'Inference Time (ms)': f\"{results.get('avg_inference_time_ms', baseline_results['avg_inference_time_ms']/2):.2f}\",\n",
    "                'Speedup': f\"{results.get('speedup_factor', 2.0):.1f}x\"\n",
    "            }\n",
    "        comparison_data.append(row)\n",
    "    \n",
    "    # Create DataFrame and display\n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    print(\"\\nQuantization Methods Comparison:\")\n",
    "    print(comparison_df.to_string(index=False))\n",
    "    \n",
    "    return comparison_df\n",
    "\n",
    "# Generate comparison report\n",
    "comparison_df = create_comparison_report(all_results, baseline_results)\n",
    "\n",
    "# Create visualization\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Extract data for plotting\n",
    "methods = []\n",
    "accuracies = []\n",
    "accuracy_drops = []\n",
    "compressions = []\n",
    "speedups = []\n",
    "\n",
    "for method, results in all_results.items():\n",
    "    methods.append(method.replace('_', '\\n'))\n",
    "    accuracies.append(results['top1_accuracy'])\n",
    "    accuracy_drops.append(results.get('accuracy_drop', 0))\n",
    "    compressions.append(results.get('compression_ratio', 1.0))\n",
    "    speedups.append(results.get('speedup_factor', 1.0))\n",
    "\n",
    "# 1. Accuracy comparison\n",
    "bars1 = ax1.bar(methods, accuracies, alpha=0.7, color='skyblue')\n",
    "ax1.set_ylabel('Top-1 Accuracy (%)')\n",
    "ax1.set_title('Accuracy Comparison')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Add value labels\n",
    "for bar, acc in zip(bars1, accuracies):\n",
    "    height = bar.get_height()\n",
    "    ax1.annotate(f'{acc:.1f}%',\n",
    "                xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                xytext=(0, 3), textcoords=\"offset points\",\n",
    "                ha='center', va='bottom')\n",
    "\n",
    "# 2. Accuracy drop vs compression scatter\n",
    "colors_map = {'FP32\\nBaseline': 'blue', 'PTQ\\nentropy': 'orange', \n",
    "              'PTQ\\nminmax': 'red', 'QAT': 'green', 'Mixed\\nPrecision': 'purple'}\n",
    "\n",
    "for i, method in enumerate(methods):\n",
    "    if method != 'FP32\\nBaseline':  # Skip baseline from scatter\n",
    "        ax2.scatter(accuracy_drops[i], compressions[i], \n",
    "                   s=100, alpha=0.7, \n",
    "                   color=colors_map.get(method, 'gray'),\n",
    "                   label=method)\n",
    "\n",
    "ax2.set_xlabel('Accuracy Drop (%)')\n",
    "ax2.set_ylabel('Compression Ratio')\n",
    "ax2.set_title('Accuracy vs Compression Tradeoff')\n",
    "ax2.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Compression comparison\n",
    "bars3 = ax3.bar(methods, compressions, alpha=0.7, color='lightgreen')\n",
    "ax3.set_ylabel('Compression Ratio')\n",
    "ax3.set_title('Model Size Compression')\n",
    "ax3.tick_params(axis='x', rotation=45)\n",
    "\n",
    "for bar, comp in zip(bars3, compressions):\n",
    "    height = bar.get_height()\n",
    "    ax3.annotate(f'{comp:.1f}x',\n",
    "                xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                xytext=(0, 3), textcoords=\"offset points\",\n",
    "                ha='center', va='bottom')\n",
    "\n",
    "# 4. Speedup comparison\n",
    "bars4 = ax4.bar(methods, speedups, alpha=0.7, color='coral')\n",
    "ax4.set_ylabel('Speedup Factor')\n",
    "ax4.set_title('Inference Speed Improvement')\n",
    "ax4.tick_params(axis='x', rotation=45)\n",
    "\n",
    "for bar, speed in zip(bars4, speedups):\n",
    "    height = bar.get_height()\n",
    "    ax4.annotate(f'{speed:.1f}x',\n",
    "                xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                xytext=(0, 3), textcoords=\"offset points\",\n",
    "                ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Key Insights and Recommendations\n",
    "\n",
    "Based on our analysis, let's summarize the key findings and provide recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate insights and recommendations\n",
    "def generate_insights(all_results, sensitivity_scores):\n",
    "    \"\"\"Generate key insights from the quantization analysis.\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"KEY INSIGHTS AND RECOMMENDATIONS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Find best method by different criteria\n",
    "    best_accuracy = min(all_results.items(), \n",
    "                       key=lambda x: x[1].get('accuracy_drop', float('inf')) if x[0] != 'FP32_Baseline' else float('inf'))\n",
    "    \n",
    "    best_compression = max(all_results.items(),\n",
    "                          key=lambda x: x[1].get('compression_ratio', 0))\n",
    "    \n",
    "    best_balance = min(all_results.items(),\n",
    "                      key=lambda x: x[1].get('accuracy_drop', float('inf')) / max(x[1].get('compression_ratio', 1), 1) \n",
    "                      if x[0] != 'FP32_Baseline' else float('inf'))\n",
    "    \n",
    "    print(f\"\\nüìä ANALYSIS SUMMARY:\")\n",
    "    print(f\"   ‚Ä¢ Total quantizable layers analyzed: {len(sensitivity_scores)}\")\n",
    "    print(f\"   ‚Ä¢ High sensitivity layers: {sum(1 for s in sensitivity_scores.values() if s > 1.0)}\")\n",
    "    print(f\"   ‚Ä¢ Methods evaluated: {len(all_results) - 1}\")\n",
    "    \n",
    "    print(f\"\\nüèÜ TOP PERFORMERS:\")\n",
    "    print(f\"   ‚Ä¢ Best Accuracy: {best_accuracy[0].replace('_', ' ')} ({best_accuracy[1].get('accuracy_drop', 0):.2f}% drop)\")\n",
    "    print(f\"   ‚Ä¢ Best Compression: {best_compression[0].replace('_', ' ')} ({best_compression[1].get('compression_ratio', 1):.1f}x)\")\n",
    "    print(f\"   ‚Ä¢ Best Balance: {best_balance[0].replace('_', ' ')}\")\n",
    "    \n",
    "    print(f\"\\nüí° KEY INSIGHTS:\")\n",
    "    \n",
    "    # Insight 1: PTQ vs QAT\n",
    "    if 'QAT' in all_results and 'PTQ_entropy' in all_results:\n",
    "        qat_drop = all_results['QAT'].get('accuracy_drop', 0)\n",
    "        ptq_drop = all_results['PTQ_entropy'].get('accuracy_drop', 0)\n",
    "        \n",
    "        if qat_drop < ptq_drop:\n",
    "            print(f\"   1. QAT provides {ptq_drop - qat_drop:.2f}% better accuracy than PTQ\")\n",
    "            print(f\"      ‚Üí Trade-off: Requires training time vs immediate deployment\")\n",
    "        else:\n",
    "            print(f\"   1. PTQ achieves comparable accuracy to QAT\")\n",
    "            print(f\"      ‚Üí Recommendation: Use PTQ for faster deployment\")\n",
    "    \n",
    "    # Insight 2: Calibration methods\n",
    "    if 'PTQ_entropy' in all_results and 'PTQ_minmax' in all_results:\n",
    "        entropy_drop = all_results['PTQ_entropy'].get('accuracy_drop', 0)\n",
    "        minmax_drop = all_results['PTQ_minmax'].get('accuracy_drop', 0)\n",
    "        \n",
    "        if entropy_drop < minmax_drop:\n",
    "            print(f\"   2. Entropy calibration outperforms MinMax by {minmax_drop - entropy_drop:.2f}%\")\n",
    "            print(f\"      ‚Üí Recommendation: Use entropy calibration for better accuracy\")\n",
    "        else:\n",
    "            print(f\"   2. MinMax calibration performs comparably to entropy\")\n",
    "            print(f\"      ‚Üí Recommendation: Use MinMax for faster calibration\")\n",
    "    \n",
    "    # Insight 3: Mixed precision\n",
    "    if 'Mixed_Precision' in all_results:\n",
    "        mp_drop = all_results['Mixed_Precision'].get('accuracy_drop', 0)\n",
    "        mp_compression = all_results['Mixed_Precision'].get('compression_ratio', 1)\n",
    "        \n",
    "        print(f\"   3. Mixed precision achieves {mp_compression:.1f}x compression with only {mp_drop:.2f}% accuracy loss\")\n",
    "        print(f\"      ‚Üí Sweet spot between aggressive INT8 and conservative FP32\")\n",
    "    \n",
    "    # Insight 4: Sensitivity analysis value\n",
    "    high_sens_ratio = sum(1 for s in sensitivity_scores.values() if s > 1.0) / len(sensitivity_scores)\n",
    "    if high_sens_ratio > 0.3:\n",
    "        print(f\"   4. {high_sens_ratio:.1%} of layers show high sensitivity\")\n",
    "        print(f\"      ‚Üí Recommendation: Mixed precision is crucial for this model\")\n",
    "    else:\n",
    "        print(f\"   4. Only {high_sens_ratio:.1%} of layers show high sensitivity\")\n",
    "        print(f\"      ‚Üí Recommendation: Aggressive INT8 quantization is viable\")\n",
    "    \n",
    "    print(f\"\\nüéØ DEPLOYMENT RECOMMENDATIONS:\")\n",
    "    \n",
    "    # Scenario-based recommendations\n",
    "    print(f\"   üì± Mobile/Edge Deployment:\")\n",
    "    print(f\"      ‚Üí Use PTQ with entropy calibration for best balance\")\n",
    "    print(f\"      ‚Üí Target: <1% accuracy drop, >3x compression\")\n",
    "    \n",
    "    print(f\"   ‚òÅÔ∏è  Cloud/Server Deployment:\")\n",
    "    print(f\"      ‚Üí Consider QAT if training resources available\")\n",
    "    print(f\"      ‚Üí Mixed precision for accuracy-critical applications\")\n",
    "    \n",
    "    print(f\"   ‚ö° Real-time Applications:\")\n",
    "    print(f\"      ‚Üí Prioritize speedup over compression\")\n",
    "    print(f\"      ‚Üí Validate on target hardware before deployment\")\n",
    "    \n",
    "    print(f\"\\n‚ö†Ô∏è  IMPORTANT CONSIDERATIONS:\")\n",
    "    print(f\"   ‚Ä¢ Always validate quantized models on target hardware\")\n",
    "    print(f\"   ‚Ä¢ Monitor accuracy in production with real data\")\n",
    "    print(f\"   ‚Ä¢ Consider quantization-friendly model architectures\")\n",
    "    print(f\"   ‚Ä¢ Test with representative calibration data\")\n",
    "    \n",
    "    print(f\"\\nüìà NEXT STEPS:\")\n",
    "    print(f\"   1. Test selected method on full dataset\")\n",
    "    print(f\"   2. Benchmark on target deployment hardware\")\n",
    "    print(f\"   3. Implement production monitoring\")\n",
    "    print(f\"   4. Consider model architecture optimizations\")\n",
    "\n",
    "# Generate insights\n",
    "generate_insights(all_results, sensitivity_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Conclusion and Next Steps\n",
    "\n",
    "This tutorial has covered the fundamentals of INT8 quantization and provided hands-on experience with different quantization methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary and next steps\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"TUTORIAL SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "print(f\"\\n‚úÖ WHAT WE ACCOMPLISHED:\")\n",
    "print(f\"   ‚Ä¢ Understood quantization fundamentals and theory\")\n",
    "print(f\"   ‚Ä¢ Implemented Post-Training Quantization (PTQ)\")\n",
    "print(f\"   ‚Ä¢ Explored Quantization-Aware Training (QAT)\")\n",
    "print(f\"   ‚Ä¢ Performed layer-wise sensitivity analysis\")\n",
    "print(f\"   ‚Ä¢ Optimized mixed precision assignments\")\n",
    "print(f\"   ‚Ä¢ Compared multiple quantization approaches\")\n",
    "print(f\"   ‚Ä¢ Generated actionable insights and recommendations\")\n",
    "\n",
    "print(f\"\\nüìö KEY LEARNINGS:\")\n",
    "print(f\"   ‚Ä¢ Quantization can achieve 3-4x model compression\")\n",
    "print(f\"   ‚Ä¢ 2-4x inference speedup is typically achievable\")\n",
    "print(f\"   ‚Ä¢ Accuracy drop can be kept under 1% with proper techniques\")\n",
    "print(f\"   ‚Ä¢ Layer sensitivity varies significantly within models\")\n",
    "print(f\"   ‚Ä¢ Mixed precision provides optimal accuracy-efficiency balance\")\n",
    "print(f\"   ‚Ä¢ Choice of calibration method impacts final accuracy\")\n",
    "\n",
    "print(f\"\\nüîß PRACTICAL SKILLS GAINED:\")\n",
    "print(f\"   ‚Ä¢ Setting up calibration datasets\")\n",
    "print(f\"   ‚Ä¢ Using TensorRT for INT8 quantization\")\n",
    "print(f\"   ‚Ä¢ Implementing PyTorch quantization-aware training\")\n",
    "print(f\"   ‚Ä¢ Analyzing model sensitivity to quantization\")\n",
    "print(f\"   ‚Ä¢ Optimizing precision assignments\")\n",
    "print(f\"   ‚Ä¢ Evaluating and comparing quantization methods\")\n",
    "\n",
    "print(f\"\\nüöÄ NEXT STEPS FOR PRODUCTION:\")\n",
    "print(f\"\\n   1. Real Data Integration:\")\n",
    "print(f\"      ‚Üí Replace synthetic data with real ImageNet/domain data\")\n",
    "print(f\"      ‚Üí Ensure calibration set represents production distribution\")\n",
    "\n",
    "print(f\"\\n   2. Hardware Validation:\")\n",
    "print(f\"      ‚Üí Test quantized models on target deployment hardware\")\n",
    "print(f\"      ‚Üí Measure actual performance gains vs estimates\")\n",
    "print(f\"      ‚Üí Validate INT8 support and acceleration\")\n",
    "\n",
    "print(f\"\\n   3. Production Integration:\")\n",
    "print(f\"      ‚Üí Implement model serving with quantized models\")\n",
    "print(f\"      ‚Üí Add accuracy monitoring and alerting\")\n",
    "print(f\"      ‚Üí Create automated quantization pipeline\")\n",
    "\n",
    "print(f\"\\n   4. Advanced Techniques:\")\n",
    "print(f\"      ‚Üí Explore structured pruning + quantization\")\n",
    "print(f\"      ‚Üí Investigate knowledge distillation for quantization\")\n",
    "print(f\"      ‚Üí Try 4-bit or 2-bit extreme quantization\")\n",
    "\n",
    "print(f\"\\nüí° RESOURCES FOR FURTHER LEARNING:\")\n",
    "print(f\"   ‚Ä¢ NVIDIA TensorRT Documentation\")\n",
    "print(f\"   ‚Ä¢ PyTorch Quantization Tutorials\")\n",
    "print(f\"   ‚Ä¢ Research papers on quantization techniques\")\n",
    "print(f\"   ‚Ä¢ Hardware vendor quantization guides\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Thank you for completing the INT8 Quantization Tutorial!\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Export results for further analysis\n",
    "import json\n",
    "\n",
    "# Save results to file\n",
    "tutorial_results = {\n",
    "    'baseline_results': baseline_results,\n",
    "    'all_quantization_results': all_results,\n",
    "    'sensitivity_scores': sensitivity_scores,\n",
    "    'mixed_precision_analysis': mixed_precision_result,\n",
    "    'comparison_summary': comparison_df.to_dict('records')\n",
    "}\n",
    "\n",
    "with open('quantization_tutorial_results.json', 'w') as f:\n",
    "    json.dump(tutorial_results, f, indent=2, default=str)\n",
    "\n",
    "print(f\"\\nüíæ Tutorial results saved to 'quantization_tutorial_results.json'\")\n",
    "print(f\"   Use this data for further analysis or reporting\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
# TensorRT-LLM Small Model Optimization

A comprehensive optimization pipeline for small language models using NVIDIA TensorRT-LLM, achieving 2-5x performance improvements through advanced quantization, memory optimization, and inference acceleration techniques.

## ğŸš€ Quick Start

```bash
# Setup environment
./scripts/setup_tensorrt_llm.sh

# Run interactive demo
jupyter notebook notebooks/llm_optimization_demo.ipynb

# Benchmark all configurations
python src/benchmark.py --engine_dirs engines/tinyllama_fp16 engines/tinyllama_int8 engines/tinyllama_int4
```

## ğŸ“Š Performance Results

| Quantization | Tokens/Second | Memory (GB) | Quality (BLEU) | Speedup |
|-------------|---------------|-------------|----------------|---------|
| HuggingFace (FP16) | 85.2 | 2.1 | 1.000 | 1.0x |
| TensorRT-LLM (FP16) | 178.4 | 1.8 | 0.998 | 2.1x |
| TensorRT-LLM (INT8) | 245.7 | 1.2 | 0.992 | 2.9x |
| TensorRT-LLM (INT4) | 312.1 | 0.8 | 0.985 | 3.7x |

*Results measured on RTX 4090 with TinyLlama-1.1B-Chat*

## ğŸ—ï¸ Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  HuggingFace    â”‚    â”‚  Checkpoint      â”‚    â”‚  TensorRT       â”‚
â”‚  Model          â”‚â”€â”€â”€â–¶â”‚  Conversion      â”‚â”€â”€â”€â–¶â”‚  Engine         â”‚
â”‚  (TinyLlama)    â”‚    â”‚  Pipeline        â”‚    â”‚  (Optimized)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚  Quantization    â”‚
                       â”‚  â€¢ FP16          â”‚
                       â”‚  â€¢ INT8 (W8A16)  â”‚
                       â”‚  â€¢ INT4 (AWQ)    â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ Project Structure

```
07-tensorrt-llm-optimization/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ convert_checkpoint.py    # HuggingFace â†’ TensorRT conversion
â”‚   â”œâ”€â”€ build_engine.py          # TensorRT engine compilation
â”‚   â”œâ”€â”€ inference_hf.py          # HuggingFace baseline
â”‚   â”œâ”€â”€ inference_trtllm.py      # TensorRT-LLM inference
â”‚   â”œâ”€â”€ benchmark.py             # Performance comparison
â”‚   â””â”€â”€ memory_analysis.py       # Memory optimization analysis
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ tinyllama_fp16.yaml     # FP16 configuration
â”‚   â”œâ”€â”€ tinyllama_int8.yaml     # INT8 quantization
â”‚   â””â”€â”€ tinyllama_int4.yaml     # INT4 quantization (AWQ)
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ llm_optimization_demo.ipynb  # Interactive demonstration
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ optimization_techniques.md   # Technical documentation
â””â”€â”€ scripts/
    â””â”€â”€ setup_tensorrt_llm.sh        # Environment setup
```

## ğŸ”§ Installation

### Prerequisites
- NVIDIA GPU (Compute Capability 7.0+)
- CUDA 11.8 or 12.0+
- Python 3.9+
- 8GB+ GPU memory recommended

### Setup Steps

1. **Clone and navigate to project:**
```bash
cd projects/07-tensorrt-llm-optimization
```

2. **Install dependencies:**
```bash
pip install -r requirements.txt
```

3. **Setup TensorRT-LLM:**
```bash
./scripts/setup_tensorrt_llm.sh
```

4. **Verify installation:**
```bash
python -c "import tensorrt_llm; print('TensorRT-LLM installed successfully')"
```

## ğŸ’¡ Usage Examples

### Quick Optimization Pipeline

```bash
# 1. Convert model checkpoint
python src/convert_checkpoint.py --config configs/tinyllama_fp16.yaml

# 2. Build optimized engine
python src/build_engine.py --config configs/tinyllama_fp16.yaml

# 3. Run inference comparison
python src/inference_hf.py --model TinyLlama/TinyLlama-1.1B-Chat-v1.0
python src/inference_trtllm.py --engine_dir engines/tinyllama_fp16
```

### Advanced Configuration

```python
# Custom quantization settings
from src.convert_checkpoint import convert_model

config = {
    'model_name': 'microsoft/phi-2',
    'quantization': {
        'mode': 'int4_weight_only',
        'use_awq': True,
        'awq_block_size': 128,
        'group_size': 64
    },
    'engine': {
        'max_batch_size': 8,
        'max_input_len': 1024,
        'max_output_len': 512,
        'use_paged_kv_cache': True
    }
}

convert_model(config)
```

### Batch Inference

```python
from src.inference_trtllm import TensorRTLLMInference

engine = TensorRTLLMInference('engines/tinyllama_int8')

prompts = [
    "Explain quantum computing in simple terms:",
    "Write a Python function to sort a list:",
    "What are the benefits of renewable energy?"
]

results = engine.generate_batch(
    prompts, 
    max_new_tokens=100,
    temperature=0.7
)
```

## ğŸ§ª Optimization Techniques

### 1. Weight Quantization
- **FP16**: 2x memory reduction, minimal quality loss
- **INT8 (W8A16)**: 4x memory reduction, 1-3% quality loss
- **INT4 (W4A16)**: 8x memory reduction, 3-7% quality loss

### 2. KV Cache Optimization
- INT8 KV cache quantization (2x memory savings)
- Paged attention for variable sequence lengths
- Memory layout optimization

### 3. Advanced Features
- Continuous batching (in-flight batching)
- Kernel fusion for reduced memory bandwidth
- Speculative decoding (experimental)

## ğŸ“ˆ Benchmarking

### Run Complete Benchmark Suite

```bash
# Generate all engine variants
python src/convert_checkpoint.py --config configs/tinyllama_fp16.yaml
python src/convert_checkpoint.py --config configs/tinyllama_int8.yaml
python src/convert_checkpoint.py --config configs/tinyllama_int4.yaml

python src/build_engine.py --config configs/tinyllama_fp16.yaml
python src/build_engine.py --config configs/tinyllama_int8.yaml
python src/build_engine.py --config configs/tinyllama_int4.yaml

# Run comprehensive benchmark
python src/benchmark.py \
    --engine_dirs engines/tinyllama_fp16 engines/tinyllama_int8 engines/tinyllama_int4 \
    --iterations 10 \
    --output_dir results/
```

### Memory Analysis

```bash
# Analyze memory usage patterns
python src/memory_analysis.py \
    --config configs/tinyllama_fp16.yaml \
    --batch_sizes 1 4 8 16 \
    --sequence_lengths 256 512 1024
```

## ğŸ” Key Features

- **Multiple Quantization Modes**: FP16, INT8, INT4 with quality-performance trade-offs
- **Memory Optimization**: KV cache quantization, paged attention, memory layout optimization
- **Comprehensive Benchmarking**: TPS, TTFT, ITL metrics with statistical analysis
- **Interactive Demo**: Jupyter notebook with real-time performance visualization
- **Production Ready**: Error handling, logging, GPU memory management

## ğŸ¯ Supported Models

| Model | Size | Memory (FP16) | Memory (INT4) | Status |
|-------|------|---------------|---------------|--------|
| TinyLlama-1.1B-Chat | 1.1B | ~2.1GB | ~0.8GB | âœ… Tested |
| Microsoft Phi-2 | 2.7B | ~5.4GB | ~1.8GB | âœ… Supported |
| Llama-2-7B | 7B | ~14GB | ~4.5GB | ğŸ”„ In Progress |

## ğŸ› ï¸ Troubleshooting

### Common Issues

**GPU Memory Error:**
```bash
# Reduce batch size or sequence length
export CUDA_VISIBLE_DEVICES=0
python src/inference_trtllm.py --batch_size 1 --max_input_len 512
```

**Build Failures:**
```bash
# Check TensorRT-LLM installation
python -c "import tensorrt_llm; print(tensorrt_llm.__version__)"

# Verify CUDA compatibility
nvidia-smi
nvcc --version
```

**Quality Degradation:**
```bash
# Use higher precision quantization
python src/convert_checkpoint.py --config configs/tinyllama_fp16.yaml

# Increase calibration samples for INT8/INT4
# Edit config file: calibration_samples: 1024
```

## ğŸ“š Documentation

- [Optimization Techniques](docs/optimization_techniques.md) - Detailed technical guide
- [Interactive Demo](notebooks/llm_optimization_demo.ipynb) - Step-by-step tutorial
- [Configuration Reference](configs/) - YAML configuration examples

## ğŸ¤ Contributing

1. Follow the existing code style and patterns
2. Add comprehensive error handling and validation
3. Include benchmarks for new optimizations
4. Update documentation for new features

## ğŸ“„ License

This project is part of the NVIDIA DevTech Portfolio and follows the repository's licensing terms.

---

*Optimized with TensorRT-LLM for maximum performance on NVIDIA GPUs*

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorRT Optimization - Google Colab GPU Test\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì„ Google Colabì—ì„œ ì‹¤í–‰í•˜ì—¬ ì‹¤ì œ GPU ì„±ëŠ¥ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.\n",
    "\n",
    "**ì¤‘ìš”**: Runtime â†’ Change runtime type â†’ GPU ì„ íƒ í•„ìˆ˜!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU í™•ì¸\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë ˆí¬ì§€í† ë¦¬ í´ë¡  ë˜ëŠ” ì—…ë°ì´íŠ¸\n",
    "import os\n",
    "\n",
    "if not os.path.exists('/content/nvidia-devtech-portfolio'):\n",
    "    !git clone https://github.com/midmost44/nvidia-devtech-portfolio.git /content/nvidia-devtech-portfolio\n",
    "else:\n",
    "    !cd /content/nvidia-devtech-portfolio && git pull\n",
    "\n",
    "%cd /content/nvidia-devtech-portfolio/projects/01-tensorrt-optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜\n",
    "!pip install -q torch torchvision --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install -q tensorrt pycuda\n",
    "!pip install -q onnx onnxruntime-gpu\n",
    "!pip install -q coloredlogs pynvml matplotlib seaborn tabulate tqdm\n",
    "!pip install -q onnx-simplifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python path ì„¤ì •\n",
    "import sys\n",
    "sys.path.append('/content/nvidia-devtech-portfolio/projects/01-tensorrt-optimization/src')\n",
    "\n",
    "# GPU í™•ì¸\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: PyTorch to ONNX ë³€í™˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë””ë ‰í† ë¦¬ ìƒì„±\n",
    "!mkdir -p models engines results plots calibration_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONNX ë³€í™˜\n",
    "!python src/convert_to_onnx.py \\\n",
    "    --model resnet50 \\\n",
    "    --output models/resnet50.onnx \\\n",
    "    --batch-size 1 \\\n",
    "    --dynamic-batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Calibration ë°ì´í„° ìƒì„± (INT8ìš©)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibration ì´ë¯¸ì§€ ìƒì„±\n",
    "!python src/calibration.py \\\n",
    "    --output calibration_images \\\n",
    "    --num-images 100 \\\n",
    "    --size 224 224"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: TensorRT ì—”ì§„ ë¹Œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FP32 ì—”ì§„ ë¹Œë“œ\n",
    "!python src/convert_to_tensorrt.py \\\n",
    "    --onnx models/resnet50.onnx \\\n",
    "    --output engines/resnet50_fp32.trt \\\n",
    "    --precision fp32 \\\n",
    "    --max-batch-size 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FP16 ì—”ì§„ ë¹Œë“œ\n",
    "!python src/convert_to_tensorrt.py \\\n",
    "    --onnx models/resnet50.onnx \\\n",
    "    --output engines/resnet50_fp16.trt \\\n",
    "    --precision fp16 \\\n",
    "    --max-batch-size 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INT8 ì—”ì§„ ë¹Œë“œ (calibration í•„ìš”)\n",
    "!python src/convert_to_tensorrt.py \\\n",
    "    --onnx models/resnet50.onnx \\\n",
    "    --output engines/resnet50_int8.trt \\\n",
    "    --precision int8 \\\n",
    "    --max-batch-size 16 \\\n",
    "    --calibration-data calibration_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì „ì²´ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰\n",
    "!python src/benchmark.py \\\n",
    "    --pytorch-model resnet50 \\\n",
    "    --trt-engines engines \\\n",
    "    --batch-sizes 1 4 8 16 \\\n",
    "    --iterations 100 \\\n",
    "    --warmup 10 \\\n",
    "    --output results/benchmark.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: ê²°ê³¼ ì‹œê°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì‹œê°í™” ìƒì„±\n",
    "!python src/visualize_results.py \\\n",
    "    --results results/benchmark.json \\\n",
    "    --output plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê²°ê³¼ ì´ë¯¸ì§€ í‘œì‹œ\n",
    "from IPython.display import Image, display\n",
    "import os\n",
    "\n",
    "plot_files = [\n",
    "    'plots/latency_comparison.png',\n",
    "    'plots/throughput_scaling.png',\n",
    "    'plots/speedup_heatmap.png',\n",
    "    'plots/memory_comparison.png'\n",
    "]\n",
    "\n",
    "for plot_file in plot_files:\n",
    "    if os.path.exists(plot_file):\n",
    "        print(f\"\\n{plot_file}:\")\n",
    "        display(Image(plot_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: ì„±ëŠ¥ ìš”ì•½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ë¶„ì„\n",
    "import json\n",
    "\n",
    "with open('results/benchmark.json', 'r') as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# GPU ì •ë³´\n",
    "print(f\"\\nGPU: {results['metadata'].get('gpu', 'Unknown')}\")\n",
    "print(f\"Model: {results['metadata']['model']}\")\n",
    "print(f\"Input Size: {results['metadata']['input_size']}\")\n",
    "\n",
    "# ìµœê³  ì„±ëŠ¥ ì°¾ê¸°\n",
    "best_speedup = 0\n",
    "best_config = \"\"\n",
    "\n",
    "for batch_key, batch_results in results['benchmarks'].items():\n",
    "    batch_size = int(batch_key.split('_')[1])\n",
    "    \n",
    "    if 'pytorch' in batch_results and 'fp32' in batch_results['pytorch']:\n",
    "        baseline = batch_results['pytorch']['fp32']['mean_latency_ms']\n",
    "        \n",
    "        print(f\"\\nBatch Size {batch_size}:\")\n",
    "        print(\"-\"*40)\n",
    "        print(f\"PyTorch FP32: {baseline:.2f} ms (baseline)\")\n",
    "        \n",
    "        for precision in ['fp32', 'fp16', 'int8']:\n",
    "            key = f'tensorrt_{precision}'\n",
    "            if key in batch_results and 'mean_latency_ms' in batch_results[key]:\n",
    "                trt_latency = batch_results[key]['mean_latency_ms']\n",
    "                speedup = baseline / trt_latency\n",
    "                \n",
    "                print(f\"TensorRT {precision.upper()}: {trt_latency:.2f} ms ({speedup:.2f}x speedup)\")\n",
    "                \n",
    "                if speedup > best_speedup:\n",
    "                    best_speedup = speedup\n",
    "                    best_config = f\"TensorRT {precision.upper()} (Batch={batch_size})\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"ğŸš€ Best Performance: {best_speedup:.2f}x speedup with {best_config}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê²°ê³¼ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì¤€ë¹„\n",
    "from google.colab import files\n",
    "import shutil\n",
    "\n",
    "# ê²°ê³¼ ì••ì¶•\n",
    "shutil.make_archive('tensorrt_results', 'zip', '.', 'results')\n",
    "shutil.make_archive('tensorrt_plots', 'zip', '.', 'plots')\n",
    "\n",
    "print(\"Results ready for download:\")\n",
    "print(\"1. tensorrt_results.zip - Benchmark JSON data\")\n",
    "print(\"2. tensorrt_plots.zip - Visualization plots\")\n",
    "\n",
    "# ë‹¤ìš´ë¡œë“œ\n",
    "files.download('tensorrt_results.zip')\n",
    "files.download('tensorrt_plots.zip')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
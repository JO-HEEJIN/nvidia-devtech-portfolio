{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch to TensorRT Optimization Demo\n",
    "\n",
    "This notebook demonstrates the complete pipeline for optimizing deep learning models using NVIDIA TensorRT.\n",
    "\n",
    "## Overview\n",
    "\n",
    "We'll walk through:\n",
    "1. Converting a PyTorch model to ONNX format\n",
    "2. Building optimized TensorRT engines (FP32, FP16, INT8)\n",
    "3. Running inference and comparing performance\n",
    "4. Visualizing benchmark results\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- NVIDIA GPU with CUDA support\n",
    "- TensorRT 8.6+\n",
    "- All dependencies from requirements.txt installed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.append('../src')\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Import our modules\n",
    "from convert_to_onnx import load_pytorch_model, export_to_onnx, validate_onnx_model\n",
    "from convert_to_tensorrt import EngineBuilder\n",
    "from calibration import generate_calibration_data, INT8EntropyCalibrator\n",
    "from inference import TensorRTInferenceEngine\n",
    "from benchmark import BenchmarkSuite, PyTorchBenchmark\n",
    "from visualize_results import BenchmarkVisualizer\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: PyTorch to ONNX Conversion\n",
    "\n",
    "First, we'll load a pretrained ResNet50 model and convert it to ONNX format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "MODEL_NAME = 'resnet50'\n",
    "BATCH_SIZE = 1\n",
    "INPUT_SIZE = (224, 224)\n",
    "CHANNELS = 3\n",
    "\n",
    "# Create directories\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "os.makedirs('../engines', exist_ok=True)\n",
    "os.makedirs('../results', exist_ok=True)\n",
    "os.makedirs('../plots', exist_ok=True)\n",
    "\n",
    "print(f\"Loading {MODEL_NAME} model...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load PyTorch model\n",
    "model = load_pytorch_model(MODEL_NAME, pretrained=True)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "\n",
    "# Model summary\n",
    "print(f\"\\nModel architecture: {MODEL_NAME}\")\n",
    "print(f\"Input shape: ({BATCH_SIZE}, {CHANNELS}, {INPUT_SIZE[0]}, {INPUT_SIZE[1]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to ONNX\n",
    "onnx_path = f'../models/{MODEL_NAME}.onnx'\n",
    "input_shape = (BATCH_SIZE, CHANNELS, *INPUT_SIZE)\n",
    "\n",
    "print(\"Exporting to ONNX...\")\n",
    "export_to_onnx(\n",
    "    model=model,\n",
    "    output_path=onnx_path,\n",
    "    input_shape=input_shape,\n",
    "    dynamic_batch=True,\n",
    "    opset_version=16\n",
    ")\n",
    "\n",
    "# Validate ONNX model\n",
    "print(\"\\nValidating ONNX model...\")\n",
    "is_valid = validate_onnx_model(onnx_path, input_shape)\n",
    "print(f\"ONNX validation: {'PASSED' if is_valid else 'FAILED'}\")\n",
    "\n",
    "# Check file size\n",
    "onnx_size = os.path.getsize(onnx_path) / (1024 * 1024)\n",
    "print(f\"ONNX model size: {onnx_size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Generate Calibration Data for INT8\n",
    "\n",
    "For INT8 quantization, we need calibration data. We'll generate synthetic data for this demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate calibration images\n",
    "calibration_dir = '../calibration_images'\n",
    "\n",
    "if not os.path.exists(calibration_dir):\n",
    "    print(\"Generating calibration data...\")\n",
    "    generate_calibration_data(\n",
    "        output_dir=calibration_dir,\n",
    "        num_images=100,  # Use 100 images for demo\n",
    "        image_size=INPUT_SIZE\n",
    "    )\n",
    "else:\n",
    "    num_existing = len(list(Path(calibration_dir).glob('*.jpg')))\n",
    "    print(f\"Using existing calibration data: {num_existing} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Build TensorRT Engines\n",
    "\n",
    "Now we'll build TensorRT engines with different precision modes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_engine_if_needed(precision, force_rebuild=False):\n",
    "    \"\"\"Build TensorRT engine if it doesn't exist.\"\"\"\n",
    "    engine_path = f'../engines/{MODEL_NAME}_{precision}.trt'\n",
    "    \n",
    "    if os.path.exists(engine_path) and not force_rebuild:\n",
    "        print(f\"{precision.upper()} engine already exists: {engine_path}\")\n",
    "        return engine_path\n",
    "    \n",
    "    print(f\"\\nBuilding {precision.upper()} engine...\")\n",
    "    print(\"This may take a few minutes...\")\n",
    "    \n",
    "    builder = EngineBuilder(verbose=False)\n",
    "    \n",
    "    # Load ONNX\n",
    "    if not builder.load_onnx(onnx_path):\n",
    "        print(f\"Failed to load ONNX for {precision}\")\n",
    "        return None\n",
    "    \n",
    "    # Create calibrator for INT8\n",
    "    calibrator = None\n",
    "    if precision == 'int8':\n",
    "        calibrator = INT8EntropyCalibrator(\n",
    "            data_dir=calibration_dir,\n",
    "            cache_file=f'../calibration_{MODEL_NAME}.cache',\n",
    "            batch_size=8,\n",
    "            max_batches=10,\n",
    "            input_shape=(CHANNELS, *INPUT_SIZE)\n",
    "        )\n",
    "    \n",
    "    # Configure builder\n",
    "    builder.configure_builder(\n",
    "        precision=precision,\n",
    "        max_workspace_size=1024,  # 1GB workspace\n",
    "        max_batch_size=16,\n",
    "        calibrator=calibrator\n",
    "    )\n",
    "    \n",
    "    # Build engine\n",
    "    engine = builder.build_engine()\n",
    "    if engine is None:\n",
    "        print(f\"Failed to build {precision} engine\")\n",
    "        return None\n",
    "    \n",
    "    # Save engine\n",
    "    builder.save_engine(engine, engine_path)\n",
    "    \n",
    "    # Clean up\n",
    "    del engine\n",
    "    del builder\n",
    "    \n",
    "    return engine_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build engines for each precision\n",
    "precisions = ['fp32', 'fp16', 'int8']\n",
    "engine_paths = {}\n",
    "\n",
    "for precision in precisions:\n",
    "    engine_path = build_engine_if_needed(precision, force_rebuild=False)\n",
    "    if engine_path:\n",
    "        engine_paths[precision] = engine_path\n",
    "        \n",
    "print(\"\\nEngine building complete!\")\n",
    "print(\"Available engines:\")\n",
    "for precision, path in engine_paths.items():\n",
    "    size = os.path.getsize(path) / (1024 * 1024)\n",
    "    print(f\"  {precision.upper()}: {size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Quick Inference Test\n",
    "\n",
    "Let's test inference with each engine to verify they work correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy input\n",
    "dummy_input = np.random.randn(1, CHANNELS, *INPUT_SIZE).astype(np.float32)\n",
    "\n",
    "print(\"Testing inference with each engine...\\n\")\n",
    "\n",
    "for precision, engine_path in engine_paths.items():\n",
    "    print(f\"Testing {precision.upper()} engine:\")\n",
    "    \n",
    "    with TensorRTInferenceEngine(engine_path, max_batch_size=16) as engine:\n",
    "        # Warmup\n",
    "        engine.warmup(num_iterations=5)\n",
    "        \n",
    "        # Run inference\n",
    "        start = time.perf_counter()\n",
    "        output = engine.infer(dummy_input)\n",
    "        end = time.perf_counter()\n",
    "        \n",
    "        latency = (end - start) * 1000\n",
    "        \n",
    "        print(f\"  Output shape: {output.shape}\")\n",
    "        print(f\"  Latency: {latency:.2f} ms\")\n",
    "        print(f\"  Output range: [{output.min():.3f}, {output.max():.3f}]\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Comprehensive Benchmarking\n",
    "\n",
    "Now let's run comprehensive benchmarks comparing PyTorch vs TensorRT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick benchmark for demo (reduced iterations)\n",
    "batch_sizes = [1, 4, 8]\n",
    "iterations = 20  # Reduced for demo\n",
    "warmup = 5\n",
    "\n",
    "print(f\"Running benchmarks for batch sizes: {batch_sizes}\")\n",
    "print(f\"Iterations per test: {iterations}\")\n",
    "print(\"\\nThis will take a few minutes...\\n\")\n",
    "\n",
    "# Initialize benchmark suite\n",
    "suite = BenchmarkSuite(\n",
    "    pytorch_model=MODEL_NAME,\n",
    "    trt_engines_dir='../engines',\n",
    "    batch_sizes=batch_sizes,\n",
    "    input_size=INPUT_SIZE,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Run benchmarks\n",
    "results = suite.run_benchmarks(iterations=iterations, warmup=warmup)\n",
    "\n",
    "# Save results\n",
    "results_path = '../results/demo_benchmark.json'\n",
    "suite.save_results(results_path)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "suite.print_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Visualize Results\n",
    "\n",
    "Let's create visualizations to better understand the performance improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create inline visualizations\n",
    "def plot_speedup_comparison(results):\n",
    "    \"\"\"Create a simple speedup comparison plot.\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    batch_sizes = results['metadata']['batch_sizes']\n",
    "    speedups = {'FP32': [], 'FP16': [], 'INT8': []}\n",
    "    \n",
    "    for batch_size in batch_sizes:\n",
    "        batch_key = f'batch_{batch_size}'\n",
    "        if batch_key not in results['benchmarks']:\n",
    "            continue\n",
    "            \n",
    "        batch_results = results['benchmarks'][batch_key]\n",
    "        \n",
    "        # Get baseline\n",
    "        baseline = None\n",
    "        if 'pytorch' in batch_results and 'fp32' in batch_results['pytorch']:\n",
    "            baseline = batch_results['pytorch']['fp32'].get('mean_latency_ms')\n",
    "            \n",
    "        if baseline:\n",
    "            for precision in ['fp32', 'fp16', 'int8']:\n",
    "                key = f'tensorrt_{precision}'\n",
    "                if key in batch_results and 'mean_latency_ms' in batch_results[key]:\n",
    "                    speedup = baseline / batch_results[key]['mean_latency_ms']\n",
    "                    speedups[precision.upper()].append(speedup)\n",
    "                else:\n",
    "                    speedups[precision.upper()].append(0)\n",
    "    \n",
    "    # Plot bars\n",
    "    x = np.arange(len(batch_sizes))\n",
    "    width = 0.25\n",
    "    \n",
    "    colors = {'FP32': '#2ecc71', 'FP16': '#27ae60', 'INT8': '#76B900'}\n",
    "    \n",
    "    for i, (precision, values) in enumerate(speedups.items()):\n",
    "        ax.bar(x + i * width, values, width, label=f'TensorRT {precision}',\n",
    "               color=colors[precision])\n",
    "    \n",
    "    ax.axhline(y=1.0, color='red', linestyle='--', label='PyTorch Baseline')\n",
    "    \n",
    "    ax.set_xlabel('Batch Size')\n",
    "    ax.set_ylabel('Speedup Factor')\n",
    "    ax.set_title('TensorRT Speedup vs PyTorch FP32 Baseline')\n",
    "    ax.set_xticks(x + width)\n",
    "    ax.set_xticklabels(batch_sizes)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_speedup_comparison(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create latency comparison\n",
    "def plot_latency_bars(results):\n",
    "    \"\"\"Create latency comparison bar plot.\"\"\"\n",
    "    batch_size = results['metadata']['batch_sizes'][0]  # Use first batch size\n",
    "    batch_key = f'batch_{batch_size}'\n",
    "    batch_results = results['benchmarks'][batch_key]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    frameworks = []\n",
    "    latencies = []\n",
    "    colors_list = []\n",
    "    \n",
    "    # PyTorch\n",
    "    if 'pytorch' in batch_results and 'fp32' in batch_results['pytorch']:\n",
    "        frameworks.append('PyTorch FP32')\n",
    "        latencies.append(batch_results['pytorch']['fp32']['mean_latency_ms'])\n",
    "        colors_list.append('#3498db')\n",
    "    \n",
    "    # TensorRT\n",
    "    for precision, color in [('fp32', '#2ecc71'), ('fp16', '#27ae60'), ('int8', '#76B900')]:\n",
    "        key = f'tensorrt_{precision}'\n",
    "        if key in batch_results and 'mean_latency_ms' in batch_results[key]:\n",
    "            frameworks.append(f'TensorRT {precision.upper()}')\n",
    "            latencies.append(batch_results[key]['mean_latency_ms'])\n",
    "            colors_list.append(color)\n",
    "    \n",
    "    bars = ax.bar(range(len(frameworks)), latencies, color=colors_list)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, lat in zip(bars, latencies):\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "               f'{lat:.1f} ms', ha='center', va='bottom')\n",
    "    \n",
    "    ax.set_xlabel('Framework & Precision')\n",
    "    ax.set_ylabel('Latency (ms)')\n",
    "    ax.set_title(f'Inference Latency Comparison (Batch Size = {batch_size})')\n",
    "    ax.set_xticks(range(len(frameworks)))\n",
    "    ax.set_xticklabels(frameworks)\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_latency_bars(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Memory Usage Analysis\n",
    "\n",
    "Let's analyze memory usage for different configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze model sizes\n",
    "print(\"Model Size Comparison:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# PyTorch model size\n",
    "pytorch_size = sum(p.numel() * p.element_size() for p in model.parameters()) / (1024 * 1024)\n",
    "print(f\"PyTorch model (FP32): {pytorch_size:.2f} MB\")\n",
    "\n",
    "# ONNX model size\n",
    "print(f\"ONNX model: {onnx_size:.2f} MB\")\n",
    "\n",
    "# TensorRT engine sizes\n",
    "for precision, path in engine_paths.items():\n",
    "    size = os.path.getsize(path) / (1024 * 1024)\n",
    "    reduction = (1 - size/pytorch_size) * 100\n",
    "    print(f\"TensorRT {precision.upper()}: {size:.2f} MB ({reduction:.1f}% reduction)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Performance Summary\n",
    "\n",
    "Let's create a summary table with all key metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary table\n",
    "def create_summary_table(results):\n",
    "    \"\"\"Create an HTML summary table.\"\"\"\n",
    "    batch_size = results['metadata']['batch_sizes'][0]\n",
    "    batch_key = f'batch_{batch_size}'\n",
    "    batch_results = results['benchmarks'][batch_key]\n",
    "    \n",
    "    # Get PyTorch baseline\n",
    "    baseline = batch_results['pytorch']['fp32']['mean_latency_ms']\n",
    "    \n",
    "    table_html = \"\"\"\n",
    "    <table style='width:100%; border-collapse: collapse;'>\n",
    "    <tr style='background-color: #76B900; color: white;'>\n",
    "        <th style='padding: 10px; border: 1px solid #ddd;'>Configuration</th>\n",
    "        <th style='padding: 10px; border: 1px solid #ddd;'>Latency (ms)</th>\n",
    "        <th style='padding: 10px; border: 1px solid #ddd;'>Speedup</th>\n",
    "        <th style='padding: 10px; border: 1px solid #ddd;'>Throughput (FPS)</th>\n",
    "    </tr>\n",
    "    \"\"\"\n",
    "    \n",
    "    # PyTorch baseline\n",
    "    table_html += f\"\"\"\n",
    "    <tr>\n",
    "        <td style='padding: 10px; border: 1px solid #ddd;'><b>PyTorch FP32</b> (Baseline)</td>\n",
    "        <td style='padding: 10px; border: 1px solid #ddd;'>{baseline:.2f}</td>\n",
    "        <td style='padding: 10px; border: 1px solid #ddd;'>1.0x</td>\n",
    "        <td style='padding: 10px; border: 1px solid #ddd;'>{batch_results['pytorch']['fp32']['throughput_fps']:.1f}</td>\n",
    "    </tr>\n",
    "    \"\"\"\n",
    "    \n",
    "    # TensorRT results\n",
    "    for precision in ['fp32', 'fp16', 'int8']:\n",
    "        key = f'tensorrt_{precision}'\n",
    "        if key in batch_results and 'mean_latency_ms' in batch_results[key]:\n",
    "            latency = batch_results[key]['mean_latency_ms']\n",
    "            speedup = baseline / latency\n",
    "            throughput = batch_results[key]['throughput_fps']\n",
    "            \n",
    "            # Highlight best performer\n",
    "            style = 'background-color: #e8f5e9;' if precision == 'int8' else ''\n",
    "            \n",
    "            table_html += f\"\"\"\n",
    "            <tr style='{style}'>\n",
    "                <td style='padding: 10px; border: 1px solid #ddd;'><b>TensorRT {precision.upper()}</b></td>\n",
    "                <td style='padding: 10px; border: 1px solid #ddd;'>{latency:.2f}</td>\n",
    "                <td style='padding: 10px; border: 1px solid #ddd;'><b>{speedup:.1f}x</b></td>\n",
    "                <td style='padding: 10px; border: 1px solid #ddd;'>{throughput:.1f}</td>\n",
    "            </tr>\n",
    "            \"\"\"\n",
    "    \n",
    "    table_html += \"</table>\"\n",
    "    \n",
    "    return HTML(table_html)\n",
    "\n",
    "display(HTML(\"<h3>Performance Summary (Batch Size = 1)</h3>\"))\n",
    "display(create_summary_table(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "1. **Performance Improvements**: TensorRT provides significant speedup over PyTorch:\n",
    "   - FP16: Typically 2-3x faster\n",
    "   - INT8: Can achieve 3-4x speedup with minimal accuracy loss\n",
    "\n",
    "2. **Memory Efficiency**: TensorRT engines use less memory:\n",
    "   - FP16: ~50% memory reduction\n",
    "   - INT8: ~75% memory reduction\n",
    "\n",
    "3. **Optimization Techniques**: TensorRT applies several optimizations:\n",
    "   - Layer fusion\n",
    "   - Kernel auto-tuning\n",
    "   - Precision calibration\n",
    "   - Memory optimization\n",
    "\n",
    "### Production Deployment Recommendations\n",
    "\n",
    "1. **Use FP16 by default**: Best balance of speed and accuracy\n",
    "2. **Consider INT8 for edge devices**: Maximum speed and minimum memory\n",
    "3. **Profile your specific hardware**: Performance varies by GPU architecture\n",
    "4. **Cache engines**: Avoid rebuild overhead in production\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Test with your own models and data\n",
    "- Experiment with different batch sizes\n",
    "- Profile on target deployment hardware\n",
    "- Integrate into production inference pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "Clean up GPU memory and resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "import gc\n",
    "\n",
    "# Clear PyTorch cache\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Force garbage collection\n",
    "gc.collect()\n",
    "\n",
    "print(\"Cleanup complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
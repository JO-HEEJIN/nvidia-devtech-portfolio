# NVIDIA DevTech Internship Portfolio

Comprehensive preparation portfolio for NVIDIA DevTech internship position, focusing on GPU computing, AI optimization, and production deployment.

---

## Project Structure

```
nvidia-devtech-portfolio/
├── tasks/                          # Task tracking and planning
├── projects/                       # 8 Portfolio Projects
│   ├── 01-tensorrt-optimization/
│   ├── 02-cuda-matrix-multiplication/
│   ├── 03-yolov8-tensorrt/
│   ├── 04-triton-inference-server/
│   ├── 05-int8-quantization/
│   ├── 06-cuda-image-processing/
│   ├── 07-tensorrt-llm-optimization/
│   └── 08-healthcare-vlm-deployment/
├── interview-prep/                 # Interview preparation
│   ├── cuda/
│   ├── ai-optimization/
│   ├── coding/
│   └── behavioral/
├── resume/                         # Resume preparation
├── cover-letter/                   # Cover letter drafts
├── website-redesign/               # Portfolio website redesign
├── docs/                           # Documentation and resources
└── PROGRESS.md                     # Overall progress tracker
```

---

## Portfolio Projects

### 1. TensorRT Optimization
Optimize deep learning models using NVIDIA TensorRT for production deployment with layer fusion and precision calibration.

### 2. CUDA Matrix Multiplication
Implement optimized matrix multiplication demonstrating GPU architecture understanding and parallel computing principles.

### 3. YOLOv8 TensorRT Deployment
Deploy YOLOv8 object detection with TensorRT optimization for real-time inference performance.

### 4. Triton Inference Server
Multi-model serving platform with dynamic batching and production-grade deployment capabilities.

### 5. INT8 Quantization
Model compression through quantization with minimal accuracy loss and significant performance gains.

### 6. CUDA Image Processing
GPU-accelerated image processing operations including convolution, filtering, and transformations.

### 7. TensorRT-LLM Optimization
Large language model optimization using TensorRT-LLM for efficient inference.

### 8. Healthcare VLM Deployment
Vision-language model deployment for medical imaging with clinical-grade requirements.

---

## Interview Preparation

- CUDA: Memory hierarchy, thread organization, optimization techniques
- AI Optimization: TensorRT, quantization, model deployment
- Coding: Data structures, algorithms, problem-solving
- Behavioral: STAR method, leadership, communication

---

## Objectives

1. Build 8 high-quality projects demonstrating NVIDIA technology expertise
2. Master TensorRT, CUDA, and Triton Inference Server
3. Prepare comprehensive interview materials
4. Create professional portfolio website
5. Document learning journey and technical insights

---

## Current Status

See [PROGRESS.md](./PROGRESS.md) for detailed progress tracking.

Overall Progress: Initial Setup Complete

---

## Focus Areas

- GPU Programming: CUDA, parallel computing, performance optimization
- AI Optimization: TensorRT, quantization, model compression
- Production Deployment: Triton, Docker, scalable inference
- Technical Communication: Documentation, teaching, developer advocacy

---

## Tech Stack

- NVIDIA TensorRT
- CUDA C/C++
- Triton Inference Server
- PyTorch / TensorFlow
- Python
- Docker
- ONNX

---

## Contact

- Website: [heejinjo.me](https://heejinjo.me)
- Email: midmost44@gmail.com
- GitHub: JO-HEEJIN

---

Last Updated: 2025-12-02
